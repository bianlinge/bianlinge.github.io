<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>配置数据源错误</title>
      <link href="/2020/03/23/%E9%85%8D%E7%BD%AE%E6%95%B0%E6%8D%AE%E6%BA%90%E9%94%99%E8%AF%AF/"/>
      <url>/2020/03/23/%E9%85%8D%E7%BD%AE%E6%95%B0%E6%8D%AE%E6%BA%90%E9%94%99%E8%AF%AF/</url>
      
        <content type="html"><![CDATA[<p>Description:</p><p>Field dataSourceTransactionManager in com.dove.core.transaction.RedisDataSoureceTransaction required a bean of type ‘org.springframework.jdbc.datasource.DataSourceTransactionManager’ that could not be found.</p><p>Action:</p><p>Consider defining a bean of type ‘org.springframework.jdbc.datasource.DataSourceTransactionManager’ in your configuration.</p><p>原因漏加了  :  @Scope(ConfigurableListableBeanFactory.SCOPE_PROTOTYPE) </p><p>因为下面服务引用的时候排除了数据源 ,所有依赖找不到</p><p>@Component<br>@Scope(ConfigurableListableBeanFactory.SCOPE_PROTOTYPE)<br>public class RedisDataSoureceTransaction {</p><p>  /**<br>     * 数据源事务管理器<br>          */<br>        @Autowired<br>        private DataSourceTransactionManager dataSourceTransactionManager;<br>    }</p><p>Description: Failed to auto-configure a DataSource: ‘spring.datasource.url’ is not specified and no embedded datasource could be auto-configured. </p><p>Reason: Failed to determine a suitable driver class</p><p> Action: Consider the following: If you want an embedded database (H2, HSQL or Derby), please put it on the classpath. If you have database settings to be loaded from a particular profile you may need to activate it (no profiles are currently active).</p><p>无法自动配置数据源:“spring.datasource.url”未指定，并且无法自动配置任何嵌入的数据源。</p><p>原因:无法确定合适的驱动程序类别</p><p>行动:</p><p>请考虑以下几点:</p><p>如果您想要一个嵌入式数据库(H2、HSQL或Derby)，请将其放在类路径中。</p><p>如果您有要从特定配置文件加载的数据库设置，您可能需要激活它(当前没有激活的配置文件)。</p><ul><li><p>没有配置对应的数据源,配置文件添加spring.datasource.url</p></li><li><p>本服务依赖其他服务的,其他服务需要配置数据源 但是本服务不需要任何数据源.所以解决方案:在服务启动类上加入</p><pre><code>@SpringBootApplication(exclude = { DataSourceAutoConfiguration.class,        DataSourceTransactionManagerAutoConfiguration.class, HibernateJpaAutoConfiguration.class })        排除数据源</code></pre></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> bug </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用命令</title>
      <link href="/2020/03/23/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/2020/03/23/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h3 id="1-mvn-jar-打包"><a href="#1-mvn-jar-打包" class="headerlink" title="1.mvn jar 打包"></a>1.mvn jar 打包</h3><hr><p>mvn install:install-file -Dfile=E:\xxl-sso-core-1.1.1-SNAPSHOT.jar   -DgroupId=com.xuxueli  -DartifactId=xxl-sso-core  -Dversion=1.1.1-SNAPSHOT -Dpackaging=jar</p><h3 id="2-windows-杀进程"><a href="#2-windows-杀进程" class="headerlink" title="2.windows 杀进程"></a>2.windows 杀进程</h3><pre class="line-numbers language-shell"><code class="language-shell">1，windows+R -->cmd       进入命令窗口2，命令行内输入 netstat   -ano|findstr  8080    8080 --指得你启动的时候的端口号 taskkill  /pid  15348 /f<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-redis-windows-服务自启动"><a href="#3-redis-windows-服务自启动" class="headerlink" title="3.redis windows 服务自启动"></a>3.redis windows 服务自启动</h3><pre class="line-numbers language-shell"><code class="language-shell">进入Redis安装目录redis-server --service-install redis.windows-service.conf --loglevel verbose<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id=""><a href="#" class="headerlink" title=""></a></h3>]]></content>
      
      
      
        <tags>
            
            <tag> windows </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>防火墙管理</title>
      <link href="/2020/03/23/%E9%98%B2%E7%81%AB%E5%A2%99%E7%AE%A1%E7%90%86/"/>
      <url>/2020/03/23/%E9%98%B2%E7%81%AB%E5%A2%99%E7%AE%A1%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h1 id="Linux-防火墙管理-CentOS-7-0"><a href="#Linux-防火墙管理-CentOS-7-0" class="headerlink" title="Linux 防火墙管理(CentOS 7.0)"></a>Linux 防火墙管理(CentOS 7.0)</h1><p>CentOS 7.0默认使用的是firewall作为防火墙</p><p>查看防火墙状态</p><pre><code>firewall-cmd --state</code></pre><p>停止firewall</p><pre><code>systemctl stop firewalld.service1</code></pre><p>禁止firewall开机启动</p><pre><code>systemctl disable firewalld.service </code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>安装Jdk1.8_CentOS7</title>
      <link href="/2020/03/23/%E5%AE%89%E8%A3%85Jdk1-8-CentOS7/"/>
      <url>/2020/03/23/%E5%AE%89%E8%A3%85Jdk1-8-CentOS7/</url>
      
        <content type="html"><![CDATA[<h1 id="Linux-安装Jdk1-8-CentOS7"><a href="#Linux-安装Jdk1-8-CentOS7" class="headerlink" title="Linux 安装Jdk1.8(CentOS7)"></a>Linux 安装Jdk1.8(CentOS7)</h1><p><strong>1.下载JDK1.8的安装包</strong></p><p> version “1.8.0_231”</p><p><strong>2.解压jdk.tar 包,一般放在/usr/java/jdk1.8.0_231</strong></p><pre><code>    tar -zxvf jdk-8u11-linux-x64.tar.gz​    mv jdk1.8.0_231 /usr/java</code></pre><p><strong>3.环境变量配置</strong></p><p>   vim /etc/profile</p><p>文末添加</p><pre><code>export JAVA_HOME=/usr/java/jdk1.8.0_11export JRE_HOME=${JAVA_HOME}/jreexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib:$CLASSPATHexport JAVA_PATH=${JAVA_HOME}/bin:${JRE_HOME}/binexport PATH=$PATH:${JAVA_PATH}</code></pre><p>使得配置文件生效</p><p>source  /etc/profile</p><p><strong>4.测试JDK安装</strong></p><p>Java -version</p><pre><code>root@localhost etc]# java -versionjava version &quot;1.8.0_231&quot;Java(TM) SE Runtime Environment (build 1.8.0_231-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.231-b11, mixed mode)</code></pre><hr><p>profile文件改错会使得Linux命令找不到,原因是环境变量Path 被改变</p><p>解决:  执行  export PATH=/bin:/usr/bin:$PATH   然后再修改原profile 进行操作</p><p>​    </p>]]></content>
      
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git基本操作命令</title>
      <link href="/2020/03/23/Git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/"/>
      <url>/2020/03/23/Git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h1 id="Command-line-instructions"><a href="#Command-line-instructions" class="headerlink" title="Command line instructions"></a>Command line instructions</h1><h2 id="Git-global-setup"><a href="#Git-global-setup" class="headerlink" title="Git global setup"></a>Git global setup</h2><pre><code>git config --global user.name &quot;dove&quot; git config --global user.email &quot;270574163@qq.com&quot; </code></pre><h2 id="Create-a-new-repository"><a href="#Create-a-new-repository" class="headerlink" title="Create a new repository"></a>Create a new repository</h2><pre><code>git clone http://5cda4bf23f2e/dove/meite-shop-parent.gitcd meite-shop-parent touch README.md git add README.md git commit -m &quot;add README&quot; git push -u origin master</code></pre><h2 id="Existing-folder"><a href="#Existing-folder" class="headerlink" title="Existing folder"></a>Existing folder</h2><pre><code>cd existing_folder git init git remote add origin http://5cda4bf23f2e/dove/meite-shop-parent.git git add . git commit -m &quot;Initial commit&quot; git push -u origin master</code></pre><h2 id="Existing-Git-repository"><a href="#Existing-Git-repository" class="headerlink" title="Existing Git repository"></a>Existing Git repository</h2><pre><code>cd existing_repo git remote rename origin old-origin git remote add origin http://5cda4bf23f2e/dove/meite-shop-parent.git git push -u origin --all git push -u origin --tags</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ELK</title>
      <link href="/2020/03/23/ELK/"/>
      <url>/2020/03/23/ELK/</url>
      
        <content type="html"><![CDATA[<h1 id="ELK"><a href="#ELK" class="headerlink" title="ELK"></a>ELK</h1><h2 id="ES原理"><a href="#ES原理" class="headerlink" title="ES原理"></a>ES原理</h2><h5 id="Elasticsearch底层采用倒排索引对文档的内容进行关键词分词，每个关键词对应多个文档出现具体位置信息，后期查询的时候直接通过分词定位到相关的文档信息，默认情况下Elasticsearch对中文分词不是很友好，可以采用第三方Ik分词器或者自定义热词。"><a href="#Elasticsearch底层采用倒排索引对文档的内容进行关键词分词，每个关键词对应多个文档出现具体位置信息，后期查询的时候直接通过分词定位到相关的文档信息，默认情况下Elasticsearch对中文分词不是很友好，可以采用第三方Ik分词器或者自定义热词。" class="headerlink" title="Elasticsearch底层采用倒排索引对文档的内容进行关键词分词，每个关键词对应多个文档出现具体位置信息，后期查询的时候直接通过分词定位到相关的文档信息，默认情况下Elasticsearch对中文分词不是很友好，可以采用第三方Ik分词器或者自定义热词。"></a>Elasticsearch底层采用倒排索引对文档的内容进行关键词分词，每个关键词对应多个文档出现具体位置信息，后期查询的时候直接通过分词定位到相关的文档信息，默认情况下Elasticsearch对中文分词不是很友好，可以采用第三方Ik分词器或者自定义热词。</h5><h2 id="logstash-v7-3-2-同步不同库的数据"><a href="#logstash-v7-3-2-同步不同库的数据" class="headerlink" title="logstash[v7.3.2]同步不同库的数据"></a>logstash[v7.3.2]同步不同库的数据</h2><hr><p>Windows 配置多管道pipelines.yml时 启动报错:</p><pre><code>ERROR: Failed to read pipelines yaml file. Location: E:/ELK/logstash-7.3.2/logstash-7.3.2/config/pipelines.yml</code></pre><blockquote><p>出错配置:</p></blockquote><pre class="line-numbers language-yml"><code class="language-yml">- pipeline.id: table1  path.config: "E:\ELK\logstash-7.3.2\logstash-7.3.2\config\mysql.conf"  pipeline.workers: 1- pipeline.id: table2  path.config: "E:\ELK\logstash-7.3.2\logstash-7.3.2\config\mysql_1.conf"  pipeline.workers: 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>正常启动配置:</p></blockquote><pre><code>- {pipeline.id: table1, path.config: &#39;E:\ELK\logstash-7.3.2\logstash-7.3.2\config\mysql.conf&#39;, pipeline.workers: 1}- {pipeline.id: table2, path.config: &#39;E:\ELK\logstash-7.3.2\logstash-7.3.2\config\mysql_1.conf&#39;, pipeline.workers: 1}</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> elk </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DTO对象</title>
      <link href="/2020/03/23/DTO%E5%AF%B9%E8%B1%A1/"/>
      <url>/2020/03/23/DTO%E5%AF%B9%E8%B1%A1/</url>
      
        <content type="html"><![CDATA[<h2 id="VO（View-Object）：视图对象"><a href="#VO（View-Object）：视图对象" class="headerlink" title="VO（View Object）：视图对象"></a>VO（View Object）：视图对象</h2><p>用于展示层，它的作用是把某个指定页面（或组件）的所有数据封装起来。</p><h2 id="DTO（Data-TransferObject）：数据传输对象"><a href="#DTO（Data-TransferObject）：数据传输对象" class="headerlink" title="DTO（Data TransferObject）：数据传输对象"></a>DTO（Data TransferObject）：数据传输对象</h2><p>这个概念来源于J2EE的设计模式，原来的目的是为了EJB的分布式应用提供粗粒度的数据实体，以减少分布式调用的次数，从而提高分布式调用的性能和降低网络负载，但在这里，我泛指用于展示层与服务层之间的数据传输对象。</p><p>接口与接口数据传输,封装接口参数,对外提供说明 2ApiModle</p><blockquote><p>inputdto</p></blockquote><blockquote><p>outputdto</p></blockquote><h2 id="DO（Domain-Object）：领域对象"><a href="#DO（Domain-Object）：领域对象" class="headerlink" title="DO（Domain Object）：领域对象"></a>DO（Domain Object）：领域对象</h2><p>就是从现实世界中抽象出来的有形或无形的业务实体。</p><p>与数据库字段意义对应</p><h2 id="PO（Persistent-Object）：持久化对象"><a href="#PO（Persistent-Object）：持久化对象" class="headerlink" title="PO（Persistent Object）：持久化对象"></a>PO（Persistent Object）：持久化对象</h2><p>它跟持久层（通常是关系型数据库）的数据结构形成一一对应的映射关系，如果持久层是关系型数据库，那么，数据表中的每个字段（或若干个）就对应PO的一个（或若干个）属性。</p><p>DTO-&gt;DO </p><p>接口传输数据转为数据库实体类</p><p>DO -&gt;DTO</p>]]></content>
      
      
      
        <tags>
            
            <tag> springmvc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>安装Redis</title>
      <link href="/2020/03/23/%E5%AE%89%E8%A3%85Redis/"/>
      <url>/2020/03/23/%E5%AE%89%E8%A3%85Redis/</url>
      
        <content type="html"><![CDATA[<h1 id="Docker-安装MySQL-Redis"><a href="#Docker-安装MySQL-Redis" class="headerlink" title="Docker 安装MySQL,Redis"></a>Docker 安装MySQL,Redis</h1><h2 id="Docker-安装Redis"><a href="#Docker-安装Redis" class="headerlink" title="Docker 安装Redis"></a>Docker 安装Redis</h2><pre><code>docker run -p 6379:6379 \--name redis \-v ~/redis/config/redis.conf:/etc/redis/redis.conf \-v ~/redis/data:/data \--privileged=true \-d redis:latest redis-server /etc/redis/redis.conf \--appendonly yes \</code></pre><p><strong>1.创建容器的时候报错:</strong></p><h5 id="WARNING-IPv4-forwarding-is-disabled-Networking-will-not-work"><a href="#WARNING-IPv4-forwarding-is-disabled-Networking-will-not-work" class="headerlink" title="WARNING: IPv4 forwarding is disabled. Networking will not work."></a>WARNING: IPv4 forwarding is disabled. Networking will not work.</h5><p><strong>解决办法：</strong></p><pre><code>vim  /usr/lib/sysctl.d/00-system.conf添加如下代码：net.ipv4.ip_forward=1重启network服务systemctl restart network完成以后，删除错误的容器，再次创建新容器，就不再报错了。</code></pre><p><strong>2.docker logs mysql/容器ID</strong></p><p>问题  chown: changing ownership of ‘/var/lib/mysql/‘: Permission denied</p><p>在docker run中加入 –privileged=true  给容器加上特定权限</p><h2 id="redis-conf"><a href="#redis-conf" class="headerlink" title="redis.conf"></a>redis.conf</h2><pre><code># Redis configuration file example.## Note that in order to read the configuration file, Redis must be# started with the file path as first argument:## ./redis-server /path/to/redis.conf# Note on units: when memory size is needed, it is possible to specify# it in the usual form of 1k 5GB 4M and so forth:## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## units are case insensitive so 1GB 1Gb 1gB are all the same.################################## INCLUDES #################################### Include one or more other config files here.  This is useful if you# have a standard template that goes to all Redis servers but also need# to customize a few per-server settings.  Include files can include# other files, so use this wisely.## Notice option &quot;include&quot; won&#39;t be rewritten by command &quot;CONFIG REWRITE&quot;# from admin or Redis Sentinel. Since Redis always uses the last processed# line as value of a configuration directive, you&#39;d better put includes# at the beginning of this file to avoid overwriting config change at runtime.## If instead you are interested in using includes to override configuration# options, it is better to use include as the last line.## include /path/to/local.conf# include /path/to/other.conf################################## MODULES ###################################### Load modules at startup. If the server is not able to load modules# it will abort. It is possible to use multiple loadmodule directives.## loadmodule /path/to/my_module.so# loadmodule /path/to/other_module.so################################## NETWORK ###################################### By default, if no &quot;bind&quot; configuration directive is specified, Redis listens# for connections from all the network interfaces available on the server.# It is possible to listen to just one or multiple selected interfaces using# the &quot;bind&quot; configuration directive, followed by one or more IP addresses.##注释掉这部分，这是限制redis只能本地访问# Examples:##bind 192.168.1.100 10.0.0.1#bind 127.0.0.1 ::1## ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the# internet, binding to all the interfaces is dangerous and will expose the# instance to everybody on the internet. So by default we uncomment the# following bind directive, that will force Redis to listen only into# the IPv4 lookback interface address (this means Redis will be able to# accept connections only from clients running into the same computer it# is running).## IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES# JUST COMMENT THE FOLLOWING LINE.# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#bind 127.0.0.1# Protected mode is a layer of security protection, in order to avoid that# Redis instances left open on the internet are accessed and exploited.## When protected mode is on and if:## 1) The server is not binding explicitly to a set of addresses using the#    &quot;bind&quot; directive.# 2) No password is configured.## The server only accepts connections from clients connecting from the# IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain# sockets.## By default protected mode is enabled. You should disable it only if# you are sure you want clients from other hosts to connect to Redis# even if no authentication is configured, nor a specific set of interfaces# are explicitly listed using the &quot;bind&quot; directive.#默认yes，开启保护模式，限制为本地访问protected-mode no# Accept connections on the specified port, default is 6379 (IANA #815344).# If port 0 is specified Redis will not listen on a TCP socket.port 6379# TCP listen() backlog.## In high requests-per-second environments you need an high backlog in order# to avoid slow clients connections issues. Note that the Linux kernel# will silently truncate it to the value of /proc/sys/net/core/somaxconn so# make sure to raise both the value of somaxconn and tcp_max_syn_backlog# in order to get the desired effect.tcp-backlog 511# Unix socket.## Specify the path for the Unix socket that will be used to listen for# incoming connections. There is no default, so Redis will not listen# on a unix socket when not specified.## unixsocket /tmp/redis.sock# unixsocketperm 700# Close the connection after a client is idle for N seconds (0 to disable)timeout 0# TCP keepalive.## If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence# of communication. This is useful for two reasons:## 1) Detect dead peers.# 2) Take the connection alive from the point of view of network#    equipment in the middle.## On Linux, the specified value (in seconds) is the period used to send ACKs.# Note that to close the connection the double of the time is needed.# On other kernels the period depends on the kernel configuration.## A reasonable value for this option is 300 seconds, which is the new# Redis default starting with Redis 3.2.1.tcp-keepalive 300################################# GENERAL ###################################### By default Redis does not run as a daemon. Use &#39;yes&#39; if you need it.# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.#默认no，改为yes意为以守护进程方式启动，可后台运行，除非kill进程（可选），改为yes会使配置文件方式启动redis失败daemonize no# If you run Redis from upstart or systemd, Redis can interact with your# supervision tree. Options:#   supervised no      - no supervision interaction#   supervised upstart - signal upstart by putting Redis into SIGSTOP mode#   supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET#   supervised auto    - detect upstart or systemd method based on#                        UPSTART_JOB or NOTIFY_SOCKET environment variables# Note: these supervision methods only signal &quot;process is ready.&quot;#       They do not enable continuous liveness pings back to your supervisor.supervised no# If a pid file is specified, Redis writes it where specified at startup# and removes it at exit.## When the server runs non daemonized, no pid file is created if none is# specified in the configuration. When the server is daemonized, the pid file# is used even if not specified, defaulting to &quot;/var/run/redis.pid&quot;.## Creating a pid file is best effort: if Redis is not able to create it# nothing bad happens, the server will start and run normally.pidfile /var/run/redis_6379.pid# Specify the server verbosity level.# This can be one of:# debug (a lot of information, useful for development/testing)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)loglevel notice# Specify the log file name. Also the empty string can be used to force# Redis to log on the standard output. Note that if you use standard# output for logging but daemonize, logs will be sent to /dev/nulllogfile &quot;&quot;# To enable logging to the system logger, just set &#39;syslog-enabled&#39; to yes,# and optionally update the other syslog parameters to suit your needs.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.# syslog-facility local0# Set the number of databases. The default database is DB 0, you can select# a different one on a per-connection basis using SELECT &lt;dbid&gt; where# dbid is a number between 0 and &#39;databases&#39;-1databases 16# By default Redis shows an ASCII art logo only when started to log to the# standard output and if the standard output is a TTY. Basically this means# that normally a logo is displayed only in interactive sessions.## However it is possible to force the pre-4.0 behavior and always show a# ASCII art logo in startup logs by setting the following option to yes.always-show-logo yes################################ SNAPSHOTTING  ################################## Save the DB on disk:##   save &lt;seconds&gt; &lt;changes&gt;##   Will save the DB if both the given number of seconds and the given#   number of write operations against the DB occurred.##   In the example below the behaviour will be to save:#   after 900 sec (15 min) if at least 1 key changed#   after 300 sec (5 min) if at least 10 keys changed#   after 60 sec if at least 10000 keys changed##   Note: you can disable saving completely by commenting out all &quot;save&quot; lines.##   It is also possible to remove all the previously configured save#   points by adding a save directive with a single empty string argument#   like in the following example:##   save &quot;&quot;save 900 1save 300 10save 60 10000# By default Redis will stop accepting writes if RDB snapshots are enabled# (at least one save point) and the latest background save failed.# This will make the user aware (in a hard way) that data is not persisting# on disk properly, otherwise chances are that no one will notice and some# disaster will happen.## If the background saving process will start working again Redis will# automatically allow writes again.## However if you have setup your proper monitoring of the Redis server# and persistence, you may want to disable this feature so that Redis will# continue to work as usual even if there are problems with disk,# permissions, and so forth.stop-writes-on-bgsave-error yes# Compress string objects using LZF when dump .rdb databases?# For default that&#39;s set to &#39;yes&#39; as it&#39;s almost always a win.# If you want to save some CPU in the saving child set it to &#39;no&#39; but# the dataset will likely be bigger if you have compressible values or keys.rdbcompression yes# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.# This makes the format more resistant to corruption but there is a performance# hit to pay (around 10%) when saving and loading RDB files, so you can disable it# for maximum performances.## RDB files created with checksum disabled have a checksum of zero that will# tell the loading code to skip the check.rdbchecksum yes# The filename where to dump the DBdbfilename dump.rdb# The working directory.## The DB will be written inside this directory, with the filename specified# above using the &#39;dbfilename&#39; configuration directive.## The Append Only File will also be created inside this directory.## Note that you must specify a directory here, not a file name.dir ./################################# REPLICATION ################################## Master-Slave replication. Use slaveof to make a Redis instance a copy of# another Redis server. A few things to understand ASAP about Redis replication.## 1) Redis replication is asynchronous, but you can configure a master to#    stop accepting writes if it appears to be not connected with at least#    a given number of slaves.# 2) Redis slaves are able to perform a partial resynchronization with the#    master if the replication link is lost for a relatively small amount of#    time. You may want to configure the replication backlog size (see the next#    sections of this file) with a sensible value depending on your needs.# 3) Replication is automatic and does not need user intervention. After a#    network partition slaves automatically try to reconnect to masters#    and resynchronize with them.## slaveof &lt;masterip&gt; &lt;masterport&gt;# If the master is password protected (using the &quot;requirepass&quot; configuration# directive below) it is possible to tell the slave to authenticate before# starting the replication synchronization process, otherwise the master will# refuse the slave request.## masterauth &lt;master-password&gt;# When a slave loses its connection with the master, or when the replication# is still in progress, the slave can act in two different ways:## 1) if slave-serve-stale-data is set to &#39;yes&#39; (the default) the slave will#    still reply to client requests, possibly with out of date data, or the#    data set may just be empty if this is the first synchronization.## 2) if slave-serve-stale-data is set to &#39;no&#39; the slave will reply with#    an error &quot;SYNC with master in progress&quot; to all the kind of commands#    but to INFO and SLAVEOF.#slave-serve-stale-data yes# You can configure a slave instance to accept writes or not. Writing against# a slave instance may be useful to store some ephemeral data (because data# written on a slave will be easily deleted after resync with the master) but# may also cause problems if clients are writing to it because of a# misconfiguration.## Since Redis 2.6 by default slaves are read-only.## Note: read only slaves are not designed to be exposed to untrusted clients# on the internet. It&#39;s just a protection layer against misuse of the instance.# Still a read only slave exports by default all the administrative commands# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve# security of read only slaves using &#39;rename-command&#39; to shadow all the# administrative / dangerous commands.slave-read-only yes# Replication SYNC strategy: disk or socket.## -------------------------------------------------------# WARNING: DISKLESS REPLICATION IS EXPERIMENTAL CURRENTLY# -------------------------------------------------------## New slaves and reconnecting slaves that are not able to continue the replication# process just receiving differences, need to do what is called a &quot;full# synchronization&quot;. An RDB file is transmitted from the master to the slaves.# The transmission can happen in two different ways:## 1) Disk-backed: The Redis master creates a new process that writes the RDB#                 file on disk. Later the file is transferred by the parent#                 process to the slaves incrementally.# 2) Diskless: The Redis master creates a new process that directly writes the#              RDB file to slave sockets, without touching the disk at all.## With disk-backed replication, while the RDB file is generated, more slaves# can be queued and served with the RDB file as soon as the current child producing# the RDB file finishes its work. With diskless replication instead once# the transfer starts, new slaves arriving will be queued and a new transfer# will start when the current one terminates.## When diskless replication is used, the master waits a configurable amount of# time (in seconds) before starting the transfer in the hope that multiple slaves# will arrive and the transfer can be parallelized.## With slow disks and fast (large bandwidth) networks, diskless replication# works better.repl-diskless-sync no# When diskless replication is enabled, it is possible to configure the delay# the server waits in order to spawn the child that transfers the RDB via socket# to the slaves.## This is important since once the transfer starts, it is not possible to serve# new slaves arriving, that will be queued for the next RDB transfer, so the server# waits a delay in order to let more slaves arrive.## The delay is specified in seconds, and by default is 5 seconds. To disable# it entirely just set it to 0 seconds and the transfer will start ASAP.repl-diskless-sync-delay 5# Slaves send PINGs to server in a predefined interval. It&#39;s possible to change# this interval with the repl_ping_slave_period option. The default value is 10# seconds.## repl-ping-slave-period 10# The following option sets the replication timeout for:## 1) Bulk transfer I/O during SYNC, from the point of view of slave.# 2) Master timeout from the point of view of slaves (data, pings).# 3) Slave timeout from the point of view of masters (REPLCONF ACK pings).## It is important to make sure that this value is greater than the value# specified for repl-ping-slave-period otherwise a timeout will be detected# every time there is low traffic between the master and the slave.## repl-timeout 60# Disable TCP_NODELAY on the slave socket after SYNC?## If you select &quot;yes&quot; Redis will use a smaller number of TCP packets and# less bandwidth to send data to slaves. But this can add a delay for# the data to appear on the slave side, up to 40 milliseconds with# Linux kernels using a default configuration.## If you select &quot;no&quot; the delay for data to appear on the slave side will# be reduced but more bandwidth will be used for replication.## By default we optimize for low latency, but in very high traffic conditions# or when the master and slaves are many hops away, turning this to &quot;yes&quot; may# be a good idea.repl-disable-tcp-nodelay no# Set the replication backlog size. The backlog is a buffer that accumulates# slave data when slaves are disconnected for some time, so that when a slave# wants to reconnect again, often a full resync is not needed, but a partial# resync is enough, just passing the portion of data the slave missed while# disconnected.## The bigger the replication backlog, the longer the time the slave can be# disconnected and later be able to perform a partial resynchronization.## The backlog is only allocated once there is at least a slave connected.## repl-backlog-size 1mb# After a master has no longer connected slaves for some time, the backlog# will be freed. The following option configures the amount of seconds that# need to elapse, starting from the time the last slave disconnected, for# the backlog buffer to be freed.## Note that slaves never free the backlog for timeout, since they may be# promoted to masters later, and should be able to correctly &quot;partially# resynchronize&quot; with the slaves: hence they should always accumulate backlog.## A value of 0 means to never release the backlog.## repl-backlog-ttl 3600# The slave priority is an integer number published by Redis in the INFO output.# It is used by Redis Sentinel in order to select a slave to promote into a# master if the master is no longer working correctly.## A slave with a low priority number is considered better for promotion, so# for instance if there are three slaves with priority 10, 100, 25 Sentinel will# pick the one with priority 10, that is the lowest.## However a special priority of 0 marks the slave as not able to perform the# role of master, so a slave with priority of 0 will never be selected by# Redis Sentinel for promotion.## By default the priority is 100.slave-priority 100# It is possible for a master to stop accepting writes if there are less than# N slaves connected, having a lag less or equal than M seconds.## The N slaves need to be in &quot;online&quot; state.## The lag in seconds, that must be &lt;= the specified value, is calculated from# the last ping received from the slave, that is usually sent every second.## This option does not GUARANTEE that N replicas will accept the write, but# will limit the window of exposure for lost writes in case not enough slaves# are available, to the specified number of seconds.## For example to require at least 3 slaves with a lag &lt;= 10 seconds use:## min-slaves-to-write 3# min-slaves-max-lag 10## Setting one or the other to 0 disables the feature.## By default min-slaves-to-write is set to 0 (feature disabled) and# min-slaves-max-lag is set to 10.# A Redis master is able to list the address and port of the attached# slaves in different ways. For example the &quot;INFO replication&quot; section# offers this information, which is used, among other tools, by# Redis Sentinel in order to discover slave instances.# Another place where this info is available is in the output of the# &quot;ROLE&quot; command of a master.## The listed IP and address normally reported by a slave is obtained# in the following way:##   IP: The address is auto detected by checking the peer address#   of the socket used by the slave to connect with the master.##   Port: The port is communicated by the slave during the replication#   handshake, and is normally the port that the slave is using to#   list for connections.## However when port forwarding or Network Address Translation (NAT) is# used, the slave may be actually reachable via different IP and port# pairs. The following two options can be used by a slave in order to# report to its master a specific set of IP and port, so that both INFO# and ROLE will report those values.## There is no need to use both the options if you need to override just# the port or the IP address.## slave-announce-ip 5.5.5.5# slave-announce-port 1234################################## SECURITY #################################### Require clients to issue AUTH &lt;PASSWORD&gt; before processing any other# commands.  This might be useful in environments in which you do not trust# others with access to the host running redis-server.## This should stay commented out for backward compatibility and because most# people do not need auth (e.g. they run their own servers).## Warning: since Redis is pretty fast an outside user can try up to# 150k passwords per second against a good box. This means that you should# use a very strong password otherwise it will be very easy to break.## requirepass foobared# requirepass Wqzs_bf0128# Command renaming.## It is possible to change the name of dangerous commands in a shared# environment. For instance the CONFIG command may be renamed into something# hard to guess so that it will still be available for internal-use tools# but not available for general clients.## Example:## rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## It is also possible to completely kill a command by renaming it into# an empty string:## rename-command CONFIG &quot;&quot;## Please note that changing the name of commands that are logged into the# AOF file or transmitted to slaves may cause problems.################################### CLIENTS ##################################### Set the max number of connected clients at the same time. By default# this limit is set to 10000 clients, however if the Redis server is not# able to configure the process file limit to allow for the specified limit# the max number of allowed clients is set to the current file limit# minus 32 (as Redis reserves a few file descriptors for internal uses).## Once the limit is reached Redis will close all the new connections sending# an error &#39;max number of clients reached&#39;.## maxclients 10000############################## MEMORY MANAGEMENT ################################# Set a memory usage limit to the specified amount of bytes.# When the memory limit is reached Redis will try to remove keys# according to the eviction policy selected (see maxmemory-policy).## If Redis can&#39;t remove keys according to the policy, or if the policy is# set to &#39;noeviction&#39;, Redis will start to reply with errors to commands# that would use more memory, like SET, LPUSH, and so on, and will continue# to reply to read-only commands like GET.## This option is usually useful when using Redis as an LRU or LFU cache, or to# set a hard memory limit for an instance (using the &#39;noeviction&#39; policy).## WARNING: If you have slaves attached to an instance with maxmemory on,# the size of the output buffers needed to feed the slaves are subtracted# from the used memory count, so that network problems / resyncs will# not trigger a loop where keys are evicted, and in turn the output# buffer of slaves is full with DELs of keys evicted triggering the deletion# of more keys, and so forth until the database is completely emptied.## In short... if you have slaves attached it is suggested that you set a lower# limit for maxmemory so that there is some free RAM on the system for slave# output buffers (but this is not needed if the policy is &#39;noeviction&#39;).## maxmemory &lt;bytes&gt;# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory# is reached. You can select among five behaviors:## volatile-lru -&gt; Evict using approximated LRU among the keys with an expire set.# allkeys-lru -&gt; Evict any key using approximated LRU.# volatile-lfu -&gt; Evict using approximated LFU among the keys with an expire set.# allkeys-lfu -&gt; Evict any key using approximated LFU.# volatile-random -&gt; Remove a random key among the ones with an expire set.# allkeys-random -&gt; Remove a random key, any key.# volatile-ttl -&gt; Remove the key with the nearest expire time (minor TTL)# noeviction -&gt; Don&#39;t evict anything, just return an error on write operations.## LRU means Least Recently Used# LFU means Least Frequently Used## Both LRU, LFU and volatile-ttl are implemented using approximated# randomized algorithms.## Note: with any of the above policies, Redis will return an error on write#       operations, when there are no suitable keys for eviction.##       At the date of writing these commands are: set setnx setex append#       incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd#       sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby#       zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby#       getset mset msetnx exec sort## The default is:## maxmemory-policy noeviction# LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated# algorithms (in order to save memory), so you can tune it for speed or# accuracy. For default Redis will check five keys and pick the one that was# used less recently, you can change the sample size using the following# configuration directive.## The default of 5 produces good enough results. 10 Approximates very closely# true LRU but costs more CPU. 3 is faster but not very accurate.## maxmemory-samples 5############################# LAZY FREEING ##################################### Redis has two primitives to delete keys. One is called DEL and is a blocking# deletion of the object. It means that the server stops processing new commands# in order to reclaim all the memory associated with an object in a synchronous# way. If the key deleted is associated with a small object, the time needed# in order to execute the DEL command is very small and comparable to most other# O(1) or O(log_N) commands in Redis. However if the key is associated with an# aggregated value containing millions of elements, the server can block for# a long time (even seconds) in order to complete the operation.## For the above reasons Redis also offers non blocking deletion primitives# such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and# FLUSHDB commands, in order to reclaim memory in background. Those commands# are executed in constant time. Another thread will incrementally free the# object in the background as fast as possible.## DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.# It&#39;s up to the design of the application to understand when it is a good# idea to use one or the other. However the Redis server sometimes has to# delete keys or flush the whole database as a side effect of other operations.# Specifically Redis deletes objects independently of a user call in the# following scenarios:## 1) On eviction, because of the maxmemory and maxmemory policy configurations,#    in order to make room for new data, without going over the specified#    memory limit.# 2) Because of expire: when a key with an associated time to live (see the#    EXPIRE command) must be deleted from memory.# 3) Because of a side effect of a command that stores data on a key that may#    already exist. For example the RENAME command may delete the old key#    content when it is replaced with another one. Similarly SUNIONSTORE#    or SORT with STORE option may delete existing keys. The SET command#    itself removes any old content of the specified key in order to replace#    it with the specified string.# 4) During replication, when a slave performs a full resynchronization with#    its master, the content of the whole database is removed in order to#    load the RDB file just transfered.## In all the above cases the default is to delete objects in a blocking way,# like if DEL was called. However you can configure each case specifically# in order to instead release memory in a non-blocking way like if UNLINK# was called, using the following configuration directives:lazyfree-lazy-eviction nolazyfree-lazy-expire nolazyfree-lazy-server-del noslave-lazy-flush no############################## APPEND ONLY MODE ################################ By default Redis asynchronously dumps the dataset on disk. This mode is# good enough in many applications, but an issue with the Redis process or# a power outage may result into a few minutes of writes lost (depending on# the configured save points).## The Append Only File is an alternative persistence mode that provides# much better durability. For instance using the default data fsync policy# (see later in the config file) Redis can lose just one second of writes in a# dramatic event like a server power outage, or a single write if something# wrong with the Redis process itself happens, but the operating system is# still running correctly.## AOF and RDB persistence can be enabled at the same time without problems.# If the AOF is enabled on startup Redis will load the AOF, that is the file# with the better durability guarantees.## Please check http://redis.io/topics/persistence for more information.appendonly no# The name of the append only file (default: &quot;appendonly.aof&quot;)appendfilename &quot;appendonly.aof&quot;# The fsync() call tells the Operating System to actually write data on disk# instead of waiting for more data in the output buffer. Some OS will really flush# data on disk, some other OS will just try to do it ASAP.## Redis supports three different modes:## no: don&#39;t fsync, just let the OS flush the data when it wants. Faster.# always: fsync after every write to the append only log. Slow, Safest.# everysec: fsync only one time every second. Compromise.## The default is &quot;everysec&quot;, as that&#39;s usually the right compromise between# speed and data safety. It&#39;s up to you to understand if you can relax this to# &quot;no&quot; that will let the operating system flush the output buffer when# it wants, for better performances (but if you can live with the idea of# some data loss consider the default persistence mode that&#39;s snapshotting),# or on the contrary, use &quot;always&quot; that&#39;s very slow but a bit safer than# everysec.## More details please check the following article:# http://antirez.com/post/redis-persistence-demystified.html## If unsure, use &quot;everysec&quot;.# appendfsync alwaysappendfsync everysec# appendfsync no# When the AOF fsync policy is set to always or everysec, and a background# saving process (a background save or AOF log background rewriting) is# performing a lot of I/O against the disk, in some Linux configurations# Redis may block too long on the fsync() call. Note that there is no fix for# this currently, as even performing fsync in a different thread will block# our synchronous write(2) call.## In order to mitigate this problem it&#39;s possible to use the following option# that will prevent fsync() from being called in the main process while a# BGSAVE or BGREWRITEAOF is in progress.## This means that while another child is saving, the durability of Redis is# the same as &quot;appendfsync none&quot;. In practical terms, this means that it is# possible to lose up to 30 seconds of log in the worst scenario (with the# default Linux settings).## If you have latency problems turn this to &quot;yes&quot;. Otherwise leave it as# &quot;no&quot; that is the safest pick from the point of view of durability.no-appendfsync-on-rewrite no# Automatic rewrite of the append only file.# Redis is able to automatically rewrite the log file implicitly calling# BGREWRITEAOF when the AOF log size grows by the specified percentage.## This is how it works: Redis remembers the size of the AOF file after the# latest rewrite (if no rewrite has happened since the restart, the size of# the AOF at startup is used).## This base size is compared to the current size. If the current size is# bigger than the specified percentage, the rewrite is triggered. Also# you need to specify a minimal size for the AOF file to be rewritten, this# is useful to avoid rewriting the AOF file even if the percentage increase# is reached but it is still pretty small.## Specify a percentage of zero in order to disable the automatic AOF# rewrite feature.auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# An AOF file may be found to be truncated at the end during the Redis# startup process, when the AOF data gets loaded back into memory.# This may happen when the system where Redis is running# crashes, especially when an ext4 filesystem is mounted without the# data=ordered option (however this can&#39;t happen when Redis itself# crashes or aborts but the operating system still works correctly).## Redis can either exit with an error when this happens, or load as much# data as possible (the default now) and start if the AOF file is found# to be truncated at the end. The following option controls this behavior.## If aof-load-truncated is set to yes, a truncated AOF file is loaded and# the Redis server starts emitting a log to inform the user of the event.# Otherwise if the option is set to no, the server aborts with an error# and refuses to start. When the option is set to no, the user requires# to fix the AOF file using the &quot;redis-check-aof&quot; utility before to restart# the server.## Note that if the AOF file will be found to be corrupted in the middle# the server will still exit with an error. This option only applies when# Redis will try to read more data from the AOF file but not enough bytes# will be found.aof-load-truncated yes# When rewriting the AOF file, Redis is able to use an RDB preamble in the# AOF file for faster rewrites and recoveries. When this option is turned# on the rewritten AOF file is composed of two different stanzas:##   [RDB file][AOF tail]## When loading Redis recognizes that the AOF file starts with the &quot;REDIS&quot;# string and loads the prefixed RDB file, and continues loading the AOF# tail.## This is currently turned off by default in order to avoid the surprise# of a format change, but will at some point be used as the default.aof-use-rdb-preamble no################################ LUA SCRIPTING  ################################ Max execution time of a Lua script in milliseconds.## If the maximum execution time is reached Redis will log that a script is# still in execution after the maximum allowed time and will start to# reply to queries with an error.## When a long running script exceeds the maximum execution time only the# SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be# used to stop a script that did not yet called write commands. The second# is the only way to shut down the server in the case a write command was# already issued by the script but the user doesn&#39;t want to wait for the natural# termination of the script.## Set it to 0 or a negative value for unlimited execution without warnings.lua-time-limit 5000################################ REDIS CLUSTER  ################################# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++# WARNING EXPERIMENTAL: Redis Cluster is considered to be stable code, however# in order to mark it as &quot;mature&quot; we need to wait for a non trivial percentage# of users to deploy it in production.# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++## Normal Redis instances can&#39;t be part of a Redis Cluster; only nodes that are# started as cluster nodes can. In order to start a Redis instance as a# cluster node enable the cluster support uncommenting the following:## cluster-enabled yes# Every cluster node has a cluster configuration file. This file is not# intended to be edited by hand. It is created and updated by Redis nodes.# Every Redis Cluster node requires a different cluster configuration file.# Make sure that instances running in the same system do not have# overlapping cluster configuration file names.## cluster-config-file nodes-6379.conf# Cluster node timeout is the amount of milliseconds a node must be unreachable# for it to be considered in failure state.# Most other internal time limits are multiple of the node timeout.## cluster-node-timeout 15000# A slave of a failing master will avoid to start a failover if its data# looks too old.## There is no simple way for a slave to actually have an exact measure of# its &quot;data age&quot;, so the following two checks are performed:## 1) If there are multiple slaves able to failover, they exchange messages#    in order to try to give an advantage to the slave with the best#    replication offset (more data from the master processed).#    Slaves will try to get their rank by offset, and apply to the start#    of the failover a delay proportional to their rank.## 2) Every single slave computes the time of the last interaction with#    its master. This can be the last ping or command received (if the master#    is still in the &quot;connected&quot; state), or the time that elapsed since the#    disconnection with the master (if the replication link is currently down).#    If the last interaction is too old, the slave will not try to failover#    at all.## The point &quot;2&quot; can be tuned by user. Specifically a slave will not perform# the failover if, since the last interaction with the master, the time# elapsed is greater than:##   (node-timeout * slave-validity-factor) + repl-ping-slave-period## So for example if node-timeout is 30 seconds, and the slave-validity-factor# is 10, and assuming a default repl-ping-slave-period of 10 seconds, the# slave will not try to failover if it was not able to talk with the master# for longer than 310 seconds.## A large slave-validity-factor may allow slaves with too old data to failover# a master, while a too small value may prevent the cluster from being able to# elect a slave at all.## For maximum availability, it is possible to set the slave-validity-factor# to a value of 0, which means, that slaves will always try to failover the# master regardless of the last time they interacted with the master.# (However they&#39;ll always try to apply a delay proportional to their# offset rank).## Zero is the only value able to guarantee that when all the partitions heal# the cluster will always be able to continue.## cluster-slave-validity-factor 10# Cluster slaves are able to migrate to orphaned masters, that are masters# that are left without working slaves. This improves the cluster ability# to resist to failures as otherwise an orphaned master can&#39;t be failed over# in case of failure if it has no working slaves.## Slaves migrate to orphaned masters only if there are still at least a# given number of other working slaves for their old master. This number# is the &quot;migration barrier&quot;. A migration barrier of 1 means that a slave# will migrate only if there is at least 1 other working slave for its master# and so forth. It usually reflects the number of slaves you want for every# master in your cluster.## Default is 1 (slaves migrate only if their masters remain with at least# one slave). To disable migration just set it to a very large value.# A value of 0 can be set but is useful only for debugging and dangerous# in production.## cluster-migration-barrier 1# By default Redis Cluster nodes stop accepting queries if they detect there# is at least an hash slot uncovered (no available node is serving it).# This way if the cluster is partially down (for example a range of hash slots# are no longer covered) all the cluster becomes, eventually, unavailable.# It automatically returns available as soon as all the slots are covered again.## However sometimes you want the subset of the cluster which is working,# to continue to accept queries for the part of the key space that is still# covered. In order to do so, just set the cluster-require-full-coverage# option to no.## cluster-require-full-coverage yes# This option, when set to yes, prevents slaves from trying to failover its# master during master failures. However the master can still perform a# manual failover, if forced to do so.## This is useful in different scenarios, especially in the case of multiple# data center operations, where we want one side to never be promoted if not# in the case of a total DC failure.## cluster-slave-no-failover no# In order to setup your cluster make sure to read the documentation# available at http://redis.io web site.########################## CLUSTER DOCKER/NAT support  ######################### In certain deployments, Redis Cluster nodes address discovery fails, because# addresses are NAT-ted or because ports are forwarded (the typical case is# Docker and other containers).## In order to make Redis Cluster working in such environments, a static# configuration where each node knows its public address is needed. The# following two options are used for this scope, and are:## * cluster-announce-ip# * cluster-announce-port# * cluster-announce-bus-port## Each instruct the node about its address, client port, and cluster message# bus port. The information is then published in the header of the bus packets# so that other nodes will be able to correctly map the address of the node# publishing the information.## If the above options are not used, the normal Redis Cluster auto-detection# will be used instead.## Note that when remapped, the bus port may not be at the fixed offset of# clients port + 10000, so you can specify any port and bus-port depending# on how they get remapped. If the bus-port is not set, a fixed offset of# 10000 will be used as usually.## Example:## cluster-announce-ip 10.1.1.5# cluster-announce-port 6379# cluster-announce-bus-port 6380################################## SLOW LOG #################################### The Redis Slow Log is a system to log queries that exceeded a specified# execution time. The execution time does not include the I/O operations# like talking with the client, sending the reply and so forth,# but just the time needed to actually execute the command (this is the only# stage of command execution where the thread is blocked and can not serve# other requests in the meantime).## You can configure the slow log with two parameters: one tells Redis# what is the execution time, in microseconds, to exceed in order for the# command to get logged, and the other parameter is the length of the# slow log. When a new command is logged the oldest one is removed from the# queue of logged commands.# The following time is expressed in microseconds, so 1000000 is equivalent# to one second. Note that a negative number disables the slow log, while# a value of zero forces the logging of every command.slowlog-log-slower-than 10000# There is no limit to this length. Just be aware that it will consume memory.# You can reclaim memory used by the slow log with SLOWLOG RESET.slowlog-max-len 128################################ LATENCY MONITOR ############################### The Redis latency monitoring subsystem samples different operations# at runtime in order to collect data related to possible sources of# latency of a Redis instance.## Via the LATENCY command this information is available to the user that can# print graphs and obtain reports.## The system only logs operations that were performed in a time equal or# greater than the amount of milliseconds specified via the# latency-monitor-threshold configuration directive. When its value is set# to zero, the latency monitor is turned off.## By default latency monitoring is disabled since it is mostly not needed# if you don&#39;t have latency issues, and collecting data has a performance# impact, that while very small, can be measured under big load. Latency# monitoring can easily be enabled at runtime using the command# &quot;CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;&quot; if needed.latency-monitor-threshold 0############################# EVENT NOTIFICATION ############################### Redis can notify Pub/Sub clients about events happening in the key space.# This feature is documented at http://redis.io/topics/notifications## For instance if keyspace events notification is enabled, and a client# performs a DEL operation on key &quot;foo&quot; stored in the Database 0, two# messages will be published via Pub/Sub:## PUBLISH __keyspace@0__:foo del# PUBLISH __keyevent@0__:del foo## It is possible to select the events that Redis will notify among a set# of classes. Every class is identified by a single character:##  K     Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.#  E     Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.#  g     Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...#  $     String commands#  l     List commands#  s     Set commands#  h     Hash commands#  z     Sorted set commands#  x     Expired events (events generated every time a key expires)#  e     Evicted events (events generated when a key is evicted for maxmemory)#  A     Alias for g$lshzxe, so that the &quot;AKE&quot; string means all the events.##  The &quot;notify-keyspace-events&quot; takes as argument a string that is composed#  of zero or multiple characters. The empty string means that notifications#  are disabled.##  Example: to enable list and generic events, from the point of view of the#           event name, use:##  notify-keyspace-events Elg##  Example 2: to get the stream of the expired keys subscribing to channel#             name __keyevent@0__:expired use:##  notify-keyspace-events Ex##  By default all notifications are disabled because most users don&#39;t need#  this feature and the feature has some overhead. Note that if you don&#39;t#  specify at least one of K or E, no events will be delivered.notify-keyspace-events &quot;&quot;############################### ADVANCED CONFIG ################################ Hashes are encoded using a memory efficient data structure when they have a# small number of entries, and the biggest entry does not exceed a given# threshold. These thresholds can be configured using the following directives.hash-max-ziplist-entries 512hash-max-ziplist-value 64# Lists are also encoded in a special way to save a lot of space.# The number of entries allowed per internal list node can be specified# as a fixed maximum size or a maximum number of elements.# For a fixed maximum size, use -5 through -1, meaning:# -5: max size: 64 Kb  &lt;-- not recommended for normal workloads# -4: max size: 32 Kb  &lt;-- not recommended# -3: max size: 16 Kb  &lt;-- probably not recommended# -2: max size: 8 Kb   &lt;-- good# -1: max size: 4 Kb   &lt;-- good# Positive numbers mean store up to _exactly_ that number of elements# per list node.# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),# but if your use case is unique, adjust the settings as necessary.list-max-ziplist-size -2# Lists may also be compressed.# Compress depth is the number of quicklist ziplist nodes from *each* side of# the list to *exclude* from compression.  The head and tail of the list# are always uncompressed for fast push/pop operations.  Settings are:# 0: disable all list compression# 1: depth 1 means &quot;don&#39;t start compressing until after 1 node into the list,#    going from either the head or tail&quot;#    So: [head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail]#    [head], [tail] will always be uncompressed; inner nodes will compress.# 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail]#    2 here means: don&#39;t compress head or head-&gt;next or tail-&gt;prev or tail,#    but compress all nodes between them.# 3: [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail]# etc.list-compress-depth 0# Sets have a special encoding in just one case: when a set is composed# of just strings that happen to be integers in radix 10 in the range# of 64 bit signed integers.# The following configuration setting sets the limit in the size of the# set in order to use this special memory saving encoding.set-max-intset-entries 512# Similarly to hashes and lists, sorted sets are also specially encoded in# order to save a lot of space. This encoding is only used when the length and# elements of a sorted set are below the following limits:zset-max-ziplist-entries 128zset-max-ziplist-value 64# HyperLogLog sparse representation bytes limit. The limit includes the# 16 bytes header. When an HyperLogLog using the sparse representation crosses# this limit, it is converted into the dense representation.## A value greater than 16000 is totally useless, since at that point the# dense representation is more memory efficient.## The suggested value is ~ 3000 in order to have the benefits of# the space efficient encoding without slowing down too much PFADD,# which is O(N) with the sparse encoding. The value can be raised to# ~ 10000 when CPU is not a concern, but space is, and the data set is# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.hll-sparse-max-bytes 3000# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in# order to help rehashing the main Redis hash table (the one mapping top-level# keys to values). The hash table implementation Redis uses (see dict.c)# performs a lazy rehashing: the more operation you run into a hash table# that is rehashing, the more rehashing &quot;steps&quot; are performed, so if the# server is idle the rehashing is never complete and some more memory is used# by the hash table.## The default is to use this millisecond 10 times every second in order to# actively rehash the main dictionaries, freeing memory when possible.## If unsure:# use &quot;activerehashing no&quot; if you have hard latency requirements and it is# not a good thing in your environment that Redis can reply from time to time# to queries with 2 milliseconds delay.## use &quot;activerehashing yes&quot; if you don&#39;t have such hard requirements but# want to free memory asap when possible.activerehashing yes# The client output buffer limits can be used to force disconnection of clients# that are not reading data from the server fast enough for some reason (a# common reason is that a Pub/Sub client can&#39;t consume messages as fast as the# publisher can produce them).## The limit can be set differently for the three different classes of clients:## normal -&gt; normal clients including MONITOR clients# slave  -&gt; slave clients# pubsub -&gt; clients subscribed to at least one pubsub channel or pattern## The syntax of every client-output-buffer-limit directive is the following:## client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;## A client is immediately disconnected once the hard limit is reached, or if# the soft limit is reached and remains reached for the specified number of# seconds (continuously).# So for instance if the hard limit is 32 megabytes and the soft limit is# 16 megabytes / 10 seconds, the client will get disconnected immediately# if the size of the output buffers reach 32 megabytes, but will also get# disconnected if the client reaches 16 megabytes and continuously overcomes# the limit for 10 seconds.## By default normal clients are not limited because they don&#39;t receive data# without asking (in a push way), but just after a request, so only# asynchronous clients may create a scenario where data is requested faster# than it can read.## Instead there is a default limit for pubsub and slave clients, since# subscribers and slaves receive data in a push fashion.## Both the hard or the soft limit can be disabled by setting them to zero.client-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60# Client query buffers accumulate new commands. They are limited to a fixed# amount by default in order to avoid that a protocol desynchronization (for# instance due to a bug in the client) will lead to unbound memory usage in# the query buffer. However you can configure it here if you have very special# needs, such us huge multi/exec requests or alike.## client-query-buffer-limit 1gb# In the Redis protocol, bulk requests, that are, elements representing single# strings, are normally limited ot 512 mb. However you can change this limit# here.## proto-max-bulk-len 512mb# Redis calls an internal function to perform many background tasks, like# closing connections of clients in timeout, purging expired keys that are# never requested, and so forth.## Not all tasks are performed with the same frequency, but Redis checks for# tasks to perform according to the specified &quot;hz&quot; value.## By default &quot;hz&quot; is set to 10. Raising the value will use more CPU when# Redis is idle, but at the same time will make Redis more responsive when# there are many keys expiring at the same time, and timeouts may be# handled with more precision.## The range is between 1 and 500, however a value over 100 is usually not# a good idea. Most users should use the default of 10 and raise this up to# 100 only in environments where very low latency is required.hz 10# When a child rewrites the AOF file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.aof-rewrite-incremental-fsync yes# Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good# idea to start with the default settings and only change them after investigating# how to improve the performances and how the keys LFU change over time, which# is possible to inspect via the OBJECT FREQ command.## There are two tunable parameters in the Redis LFU implementation: the# counter logarithm factor and the counter decay time. It is important to# understand what the two parameters mean before changing them.## The LFU counter is just 8 bits per key, it&#39;s maximum value is 255, so Redis# uses a probabilistic increment with logarithmic behavior. Given the value# of the old counter, when a key is accessed, the counter is incremented in# this way:## 1. A random number R between 0 and 1 is extracted.# 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).# 3. The counter is incremented only if R &lt; P.## The default lfu-log-factor is 10. This is a table of how the frequency# counter changes with a different number of accesses with different# logarithmic factors:## +--------+------------+------------+------------+------------+------------+# | factor | 100 hits   | 1000 hits  | 100K hits  | 1M hits    | 10M hits   |# +--------+------------+------------+------------+------------+------------+# | 0      | 104        | 255        | 255        | 255        | 255        |# +--------+------------+------------+------------+------------+------------+# | 1      | 18         | 49         | 255        | 255        | 255        |# +--------+------------+------------+------------+------------+------------+# | 10     | 10         | 18         | 142        | 255        | 255        |# +--------+------------+------------+------------+------------+------------+# | 100    | 8          | 11         | 49         | 143        | 255        |# +--------+------------+------------+------------+------------+------------+## NOTE: The above table was obtained by running the following commands:##   redis-benchmark -n 1000000 incr foo#   redis-cli object freq foo## NOTE 2: The counter initial value is 5 in order to give new objects a chance# to accumulate hits.## The counter decay time is the time, in minutes, that must elapse in order# for the key counter to be divided by two (or decremented if it has a value# less &lt;= 10).## The default value for the lfu-decay-time is 1. A Special value of 0 means to# decay the counter every time it happens to be scanned.## lfu-log-factor 10# lfu-decay-time 1########################### ACTIVE DEFRAGMENTATION ######################### WARNING THIS FEATURE IS EXPERIMENTAL. However it was stress tested# even in production and manually tested by multiple engineers for some# time.## What is active defragmentation?# -------------------------------## Active (online) defragmentation allows a Redis server to compact the# spaces left between small allocations and deallocations of data in memory,# thus allowing to reclaim back memory.## Fragmentation is a natural process that happens with every allocator (but# less so with Jemalloc, fortunately) and certain workloads. Normally a server# restart is needed in order to lower the fragmentation, or at least to flush# away all the data and create it again. However thanks to this feature# implemented by Oran Agra for Redis 4.0 this process can happen at runtime# in an &quot;hot&quot; way, while the server is running.## Basically when the fragmentation is over a certain level (see the# configuration options below) Redis will start to create new copies of the# values in contiguous memory regions by exploiting certain specific Jemalloc# features (in order to understand if an allocation is causing fragmentation# and to allocate it in a better place), and at the same time, will release the# old copies of the data. This process, repeated incrementally for all the keys# will cause the fragmentation to drop back to normal values.## Important things to understand:## 1. This feature is disabled by default, and only works if you compiled Redis#    to use the copy of Jemalloc we ship with the source code of Redis.#    This is the default with Linux builds.## 2. You never need to enable this feature if you don&#39;t have fragmentation#    issues.## 3. Once you experience fragmentation, you can enable this feature when#    needed with the command &quot;CONFIG SET activedefrag yes&quot;.## The configuration parameters are able to fine tune the behavior of the# defragmentation process. If you are not sure about what they mean it is# a good idea to leave the defaults untouched.# Enabled active defragmentation# activedefrag yes# Minimum amount of fragmentation waste to start active defrag# active-defrag-ignore-bytes 100mb# Minimum percentage of fragmentation to start active defrag# active-defrag-threshold-lower 10# Maximum percentage of fragmentation at which we use maximum effort# active-defrag-threshold-upper 100# Minimal effort for defrag in CPU percentage# active-defrag-cycle-min 25# Maximal effort for defrag in CPU percentage# active-defrag-cycle-max 75</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>安装MySQL</title>
      <link href="/2020/03/23/%E5%AE%89%E8%A3%85MySQL/"/>
      <url>/2020/03/23/%E5%AE%89%E8%A3%85MySQL/</url>
      
        <content type="html"><![CDATA[<h1 id="Docker-安装MySQL-Redis"><a href="#Docker-安装MySQL-Redis" class="headerlink" title="Docker 安装MySQL,Redis"></a>Docker 安装MySQL,Redis</h1><h2 id="Docker-安装MySQL"><a href="#Docker-安装MySQL" class="headerlink" title="Docker 安装MySQL"></a>Docker 安装MySQL</h2><pre><code>docker run -d --name mysql \           -v ~/mysql/data:/var/lib/mysql \           -e MYSQL_ROOT_PASSWORD=root \           --privileged=true \           -p 3306:3306 \           mysql:5.7</code></pre><hr><p><strong>docker logs mysql/容器ID</strong></p><p>问题  chown: changing ownership of ‘/var/lib/mysql/‘: Permission denied</p><p>在docker run中加入 –privileged=true  给容器加上特定权限</p><h2 id="Docker-安装Redis"><a href="#Docker-安装Redis" class="headerlink" title="Docker 安装Redis"></a>Docker 安装Redis</h2><pre><code>docker run -p 6379:6379 \--name redis \-v ~/redis/config/redis.conf:/etc/redis/redis.conf \-v ~/redis/data:/data \--privileged=true \-d redis:latest redis-server /etc/redis/redis.conf \--appendonly yes \</code></pre><hr><h3 id="创建容器的时候报错："><a href="#创建容器的时候报错：" class="headerlink" title="创建容器的时候报错："></a><strong><em>创建容器的时候报错：</em></strong></h3><h5 id="WARNING-IPv4-forwarding-is-disabled-Networking-will-not-work"><a href="#WARNING-IPv4-forwarding-is-disabled-Networking-will-not-work" class="headerlink" title="WARNING: IPv4 forwarding is disabled. Networking will not work."></a>WARNING: IPv4 forwarding is disabled. Networking will not work.</h5><h3 id="解决办法："><a href="#解决办法：" class="headerlink" title="解决办法："></a>解决办法：</h3><pre><code>vim  /usr/lib/sysctl.d/00-system.conf添加如下代码：net.ipv4.ip_forward=1重启network服务systemctl restart network完成以后，删除错误的容器，再次创建新容器，就不再报错了。</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>私服nexus3</title>
      <link href="/2020/03/23/%E7%A7%81%E6%9C%8Dnexus3/"/>
      <url>/2020/03/23/%E7%A7%81%E6%9C%8Dnexus3/</url>
      
        <content type="html"><![CDATA[<h1 id="Docker-安装Maven-私服-nexus3"><a href="#Docker-安装Maven-私服-nexus3" class="headerlink" title="Docker 安装Maven 私服(nexus3)"></a>Docker 安装Maven 私服(nexus3)</h1><p>Maven 私服仓库 公司内网<br>1.缓存企业级jar包<br>2.微服务开发 不会直接引入服务源码 使用私服下载jar包</p><hr><p>maven私服的作用zip<br>管理微服务接口进行RPC通讯<br>采用的是分布式开发模式,生成者将自己的服务发布到maven私服仓库中,<br>消费者直接从maven私服中下载对应的服务接口 在微服务中 消费者和生产者通讯</p><hr><p>本地仓库–&gt;私服仓库–&gt;中央仓库</p><hr><pre><code>1.docker pull sonatype/nexus32.docker run -d -p 8081:8081 --name nexus -v/root/nexus-data:/var/nexus-data --restart=always sonatype/nexus33.访问ip:8081 admin/admin123 密码不正确</code></pre><h4 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题:"></a>遇到的问题:</h4><p><strong>admin/admin123 密码不正确</strong></p><h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法:"></a>解决方法:</h4><pre><code>docker exec -it 容器id bashadmin.passwordvi admin.password 获取密码</code></pre><hr><h4 id="打包时问题"><a href="#打包时问题" class="headerlink" title="打包时问题"></a>打包时问题</h4><p> 1.ReasonPhrase:Repository version policy: RELEASE does not allow metadata in p</p><p> 2.The packaging for this project did not assign a file to the build artifact</p><h4 id="解决方法-1"><a href="#解决方法-1" class="headerlink" title="解决方法:"></a>解决方法:</h4><p>开发时版本有正式版和测试版,测试版就是1.0-SNAPSHOT.  1.0-RELEASE 正式版本<br> 区别是:<br> 你本地有正式版maven就直接使用正式版,不会去远程仓库找.<br> 测试版就算本地有也会去远程仓库找,因为测试版会自动找最新的版本来使用.</p><p> 一句话,正式版maven直接使用已有的,测试版maven使用最新的.<br> 使用mvn deploy发布到远程仓库时报错 Return code is: 400</p><p>原因：</p><p>1.nexus的repository分三种类型：Hosted、 Proxy和Virtual，<br>另外还有一个repository group(仓库组)用于对多个仓库进行组合。<br>部署的时候只能部署到Hosted类型的仓库中，如果是其他类型就会出现这个400错误。</p><p>2.默认情况下部署构件到Releases仓库中有时也会出现400错误，<br>这个原因就像上面提到的那样，Nexus中 Releases仓库默认的Deployment Policy是“Disable Redeploy”，<br> 所以无论你在settings.xml文件中将server的username设置为deployment还是使用admin都是无 法部署的，就会出现这个400错误。</p><p>3.Nexus中 Releases仓库Respository PolicySnapshot是“Release”<br>      Snapshot仓库Respository PolicySnapshot是“Snapshot” 如果设置反了或错了也是无法部署的。</p><p>4.如果你Snapshot可以发布，但是releases却发布不了，可能是<version>1.0-SNAPSHOT</version>类似这样的，version中包含了-SNAPSHOT，所以release发布不了，也会返回400错误。</p>]]></content>
      
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>安装Gitlab</title>
      <link href="/2020/03/23/%E5%AE%89%E8%A3%85Gitlab/"/>
      <url>/2020/03/23/%E5%AE%89%E8%A3%85Gitlab/</url>
      
        <content type="html"><![CDATA[<h1 id="Docker-安装Gitlab"><a href="#Docker-安装Gitlab" class="headerlink" title="Docker 安装Gitlab"></a>Docker 安装Gitlab</h1><pre><code>docker run --name=&#39;gitlab&#39; -d \--net=gitlab_net \--publish 8443:443 \--publish 8090:80 \--restart unless-stopped \--volume /mnt/gitlab/etc:/etc/gitlab \--volume /mnt/gitlab/log:/var/log/gitlab \--volume /mnt/gitlab/data:/var/opt/gitlab \gitlab/gitlab-ce:11.0.1-ce.0</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>datasource</title>
      <link href="/2020/03/23/datasource/"/>
      <url>/2020/03/23/datasource/</url>
      
        <content type="html"><![CDATA[<p>Description:</p><p>Field dataSourceTransactionManager in com.dove.core.transaction.RedisDataSoureceTransaction required a bean of type ‘org.springframework.jdbc.datasource.DataSourceTransactionManager’ that could not be found.</p><p>Action:</p><p>Consider defining a bean of type ‘org.springframework.jdbc.datasource.DataSourceTransactionManager’ in your configuration.</p><p>原因漏加了  :  @Scope(ConfigurableListableBeanFactory.SCOPE_PROTOTYPE) </p><p>因为下面服务引用的时候排除了数据源 ,所有依赖找不到</p><p>@Component<br>@Scope(ConfigurableListableBeanFactory.SCOPE_PROTOTYPE)<br>public class RedisDataSoureceTransaction {</p><p>  /**<br>     * 数据源事务管理器<br>          */<br>        @Autowired<br>        private DataSourceTransactionManager dataSourceTransactionManager;<br>    }</p><p>Description: Failed to auto-configure a DataSource: ‘spring.datasource.url’ is not specified and no embedded datasource could be auto-configured. </p><p>Reason: Failed to determine a suitable driver class</p><p> Action: Consider the following: If you want an embedded database (H2, HSQL or Derby), please put it on the classpath. If you have database settings to be loaded from a particular profile you may need to activate it (no profiles are currently active).</p><p>无法自动配置数据源:“spring.datasource.url”未指定，并且无法自动配置任何嵌入的数据源。</p><p>原因:无法确定合适的驱动程序类别</p><p>行动:</p><p>请考虑以下几点:</p><p>如果您想要一个嵌入式数据库(H2、HSQL或Derby)，请将其放在类路径中。</p><p>如果您有要从特定配置文件加载的数据库设置，您可能需要激活它(当前没有激活的配置文件)。</p><ul><li><p>没有配置对应的数据源,配置文件添加spring.datasource.url</p></li><li><p>本服务依赖其他服务的,其他服务需要配置数据源 但是本服务不需要任何数据源.所以解决方案:在服务启动类上加入</p><pre><code>@SpringBootApplication(exclude = { DataSourceAutoConfiguration.class,        DataSourceTransactionManagerAutoConfiguration.class, HibernateJpaAutoConfiguration.class })        排除数据源</code></pre></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> bug </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>@ModelAtrribute</title>
      <link href="/2020/03/23/ModelAtrribute/"/>
      <url>/2020/03/23/ModelAtrribute/</url>
      
        <content type="html"><![CDATA[<h2 id="SpringMVC-ModelAtrribute"><a href="#SpringMVC-ModelAtrribute" class="headerlink" title="SpringMVC -@ModelAtrribute"></a>SpringMVC -@ModelAtrribute</h2><h3 id="1-不使用-ModelAtrribute"><a href="#1-不使用-ModelAtrribute" class="headerlink" title="1.不使用@ModelAtrribute"></a>1.不使用@ModelAtrribute</h3><pre class="line-numbers language-java"><code class="language-java"><span class="token annotation punctuation">@RequestMapping</span><span class="token punctuation">(</span><span class="token string">"/save"</span><span class="token punctuation">)</span><span class="token keyword">public</span> String <span class="token function">save</span><span class="token punctuation">(</span>User user<span class="token punctuation">)</span> <span class="token punctuation">{</span>    user<span class="token punctuation">.</span><span class="token function">setUsername</span><span class="token punctuation">(</span><span class="token string">"U love me"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    userService<span class="token punctuation">.</span><span class="token function">save</span><span class="token punctuation">(</span>user<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token string">"result"</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​        执行此方法时会将key为”user”(注意:这里即使参数名称是user1,key一样还是”user”),value为user的对象加入到model.在jsp页面可以通过${user.id}和${user.name}得到值的,即上面方法和下面方法是相当的.</p><pre class="line-numbers language-java"><code class="language-java"><span class="token annotation punctuation">@RequestMapping</span><span class="token punctuation">(</span><span class="token string">"/save"</span><span class="token punctuation">)</span><span class="token keyword">public</span> String <span class="token function">save</span><span class="token punctuation">(</span>Model model<span class="token punctuation">,</span><span class="token keyword">int</span> id<span class="token punctuation">,</span>String username<span class="token punctuation">)</span> <span class="token punctuation">{</span>    User user<span class="token operator">=</span><span class="token keyword">new</span> <span class="token class-name">User</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">//这里是通过反射从request里面拿值再set到user</span>    user<span class="token punctuation">.</span><span class="token function">setId</span><span class="token punctuation">(</span>id<span class="token punctuation">)</span><span class="token punctuation">;</span>    user<span class="token punctuation">.</span><span class="token function">setUsername</span><span class="token punctuation">(</span>username<span class="token punctuation">)</span><span class="token punctuation">;</span>    model<span class="token punctuation">.</span><span class="token function">addAttribute</span><span class="token punctuation">(</span><span class="token string">"user"</span><span class="token punctuation">,</span>user<span class="token punctuation">)</span><span class="token punctuation">;</span>    user<span class="token punctuation">.</span><span class="token function">setUsername</span><span class="token punctuation">(</span><span class="token string">"U love me"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    userService<span class="token punctuation">.</span><span class="token function">save</span><span class="token punctuation">(</span>user<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token string">"result"</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-在方法上使用-ModelAttribute"><a href="#2-在方法上使用-ModelAttribute" class="headerlink" title="2.在方法上使用@ModelAttribute"></a>2.在方法上使用@ModelAttribute</h3><pre class="line-numbers language-java"><code class="language-java"><span class="token annotation punctuation">@ModelAttribute</span><span class="token punctuation">(</span><span class="token string">"user1"</span><span class="token punctuation">)</span><span class="token keyword">public</span> User <span class="token function">addUser</span><span class="token punctuation">(</span>User user<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">return</span> <span class="token keyword">new</span> <span class="token class-name">User</span><span class="token punctuation">(</span><span class="token number">520</span><span class="token punctuation">,</span><span class="token string">"I love U"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>假设此方法是写在UserController内,那么执行UserController内带有@RequestMapping的方法之前,都会先执行此addUser方法.</p><p>并且执行addUser过程中会添加两个对象到model,</p><p>先将key为”user”的对象(由addUser方法的User user引起的),</p><p>再添加key为”user1”的对象(由注解@ModelAttribute(“user1”)引起的).</p><h3 id="3-在方法参数上使用-ModelAttribute"><a href="#3-在方法参数上使用-ModelAttribute" class="headerlink" title="3.在方法参数上使用@ModelAttribute."></a>3.在方法参数上使用@ModelAttribute.</h3><pre class="line-numbers language-java"><code class="language-java"><span class="token annotation punctuation">@RequestMapping</span><span class="token punctuation">(</span><span class="token string">"/save"</span><span class="token punctuation">)</span><span class="token keyword">public</span> String <span class="token function">save</span><span class="token punctuation">(</span><span class="token annotation punctuation">@ModelAttribute</span> User user<span class="token punctuation">)</span> <span class="token punctuation">{</span>    user<span class="token punctuation">.</span><span class="token function">setUsername</span><span class="token punctuation">(</span><span class="token string">"U love me"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    userService<span class="token punctuation">.</span><span class="token function">save</span><span class="token punctuation">(</span>user<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token string">"result"</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>此方法会先从model去获取key为”user”的对象,如果获取不到会通过反射实例化一个User对象,</p><p>再从request里面拿值set到这个对象,然后把这个User对象添加到model(其中key为”user”).<br>使用了@ModelAttribute可修改这个key,不一定是”user”,此情况下,用与不用@ModelAttribute没有区别.</p><h3 id="4-在方法和方法参数上结合使用-ModelAttribute-即上面两步的两个方法都添加UserController-如下"><a href="#4-在方法和方法参数上结合使用-ModelAttribute-即上面两步的两个方法都添加UserController-如下" class="headerlink" title="4.在方法和方法参数上结合使用@ModelAttribute,即上面两步的两个方法都添加UserController,如下:"></a>4.在方法和方法参数上结合使用@ModelAttribute,即上面两步的两个方法都添加UserController,如下:</h3><pre class="line-numbers language-java"><code class="language-java"><span class="token annotation punctuation">@ModelAttribute</span><span class="token punctuation">(</span><span class="token string">"user1"</span><span class="token punctuation">)</span><span class="token keyword">public</span> User <span class="token function">addUser</span><span class="token punctuation">(</span>User user<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">return</span> <span class="token keyword">new</span> <span class="token class-name">User</span><span class="token punctuation">(</span><span class="token number">520</span><span class="token punctuation">,</span><span class="token string">"I love U"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token annotation punctuation">@RequestMapping</span><span class="token punctuation">(</span><span class="token string">"/save"</span><span class="token punctuation">)</span><span class="token keyword">public</span> String <span class="token function">save</span><span class="token punctuation">(</span><span class="token annotation punctuation">@ModelAttribute</span> User user<span class="token punctuation">)</span> <span class="token punctuation">{</span>    user<span class="token punctuation">.</span><span class="token function">setUsername</span><span class="token punctuation">(</span><span class="token string">"U love me"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    userService<span class="token punctuation">.</span><span class="token function">save</span><span class="token punctuation">(</span>user<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token string">"result"</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>假设要执行保存用户操作,根据一分析可知,</p><p>先执行完会产生两个User类型的对象(一个key是”user”,另一个key是”user1”)添加到model,</p><p>再执行save方法,此时会先从model去找key为”user”的对象,</p><p>能找到再从request取值set到这个User对象.最后返回到jsp页面,</p><p>model里也只有两个User类型对象.<br>再来个小小假设,将上面@ModelAttribute(“user1”)的user1改为user,其它不变.虽然执行了addUser方法,那么执行到save方法内,user对象的字段值还是来源于请求,最后返回到jsp页面,model里也只有一个User类型对象.</p>]]></content>
      
      
      
        <tags>
            
            <tag> springmvc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringApplicationContext 容器创建过程</title>
      <link href="/2020/01/14/SpringApplicationContext-%E5%AE%B9%E5%99%A8%E5%88%9B%E5%BB%BA%E8%BF%87%E7%A8%8B/"/>
      <url>/2020/01/14/SpringApplicationContext-%E5%AE%B9%E5%99%A8%E5%88%9B%E5%BB%BA%E8%BF%87%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="Spring-ApplicationContext-容器创建过程"><a href="#Spring-ApplicationContext-容器创建过程" class="headerlink" title="Spring ApplicationContext 容器创建过程"></a>Spring ApplicationContext 容器创建过程</h1><pre class="line-numbers language-java"><code class="language-java"><span class="token function">refresh</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">synchronized</span> <span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">.</span>startupShutdownMonitor<span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token comment" spellcheck="true">// Prepare this context for refreshing.</span>            <span class="token comment" spellcheck="true">//刷新前的预处理</span>            <span class="token number">1</span><span class="token punctuation">.</span><span class="token function">prepareRefresh</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                     <span class="token operator">>>></span><span class="token operator">>>></span><span class="token operator">></span><span class="token punctuation">[</span>                    <span class="token comment" spellcheck="true">// Initialize any placeholder property sources in the context environment</span>                    <span class="token comment" spellcheck="true">// 初始化一些属性设置;子类自定义个性化属性设置方法</span>                    <span class="token function">initPropertySources</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token comment" spellcheck="true">// Validate that all properties marked as required are resolvable</span>                    <span class="token comment" spellcheck="true">// see ConfigurablePropertyResolver#setRequiredProperties</span>                    <span class="token comment" spellcheck="true">//校验属性的合法等</span>                    <span class="token function">getEnvironment</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">validateRequiredProperties</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token comment" spellcheck="true">// Allow for the collection of early ApplicationEvents,</span>                    <span class="token comment" spellcheck="true">// to be published once the multicaster is available...</span>                    <span class="token comment" spellcheck="true">//保存容器中早期的事件</span>                    <span class="token keyword">this</span><span class="token punctuation">.</span>earlyApplicationEvents <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">LinkedHashSet</span><span class="token operator">&lt;</span>ApplicationEvent<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token punctuation">]</span>            <span class="token comment" spellcheck="true">// Tell the subclass to refresh the internal bean factory.</span>            <span class="token comment" spellcheck="true">//获取BeanFactory </span>            <span class="token number">2</span><span class="token punctuation">.</span>ConfigurableListableBeanFactory beanFactory <span class="token operator">=</span> <span class="token function">obtainFreshBeanFactory</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token operator">>>></span><span class="token operator">>>></span><span class="token operator">></span><span class="token punctuation">[</span>                    <span class="token comment" spellcheck="true">//刷新[创建]BeanFacatory DefaultListableBeanFactory 对象</span>                    <span class="token function">refreshBeanFactory</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token comment" spellcheck="true">//返回[获取]DefaultListableBeanFactory</span>                    ConfigurableListableBeanFactory beanFactory <span class="token operator">=</span> <span class="token function">getBeanFactory</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token keyword">if</span> <span class="token punctuation">(</span>logger<span class="token punctuation">.</span><span class="token function">isDebugEnabled</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                        logger<span class="token punctuation">.</span><span class="token function">debug</span><span class="token punctuation">(</span><span class="token string">"Bean factory for "</span> <span class="token operator">+</span> <span class="token function">getDisplayName</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">": "</span> <span class="token operator">+</span> beanFactory<span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token punctuation">}</span>                    <span class="token keyword">return</span> beanFactory<span class="token punctuation">;</span>                    <span class="token punctuation">]</span>            <span class="token comment" spellcheck="true">// Prepare the bean factory for use in this context.</span>            <span class="token comment" spellcheck="true">// BeanFactory 与准备工作 属性设置(BeanFactory的类加载器 支持表达式解析器)</span>            <span class="token number">3</span><span class="token punctuation">.</span><span class="token function">prepareBeanFactory</span><span class="token punctuation">(</span>beanFactory<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token keyword">try</span> <span class="token punctuation">{</span>                <span class="token comment" spellcheck="true">// Allows post-processing of the bean factory in context subclasses.</span>                <span class="token comment" spellcheck="true">//BeanFacatory准备工作完成的后置处理的工作</span>                <span class="token number">4</span><span class="token punctuation">.</span><span class="token function">postProcessBeanFactory</span><span class="token punctuation">(</span>beanFactory<span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token comment" spellcheck="true">// Invoke factory processors registered as beans in the context.</span>                <span class="token comment" spellcheck="true">/*执行 BeanFactoryPostProcessors的方法 --BeanFactory 的后置 处理器.                在BeanFactory标准初始化之后执行,就是前4步的操作                两个接口:BeanFactoryPostProcessor,BeanDefinitionRegistryPostProcessor 是BeanFactoryPostProcessor的子接口                先执行子接口BeanDefinitionRegistryPostProcessor后执行BeanFactoryPostProcessor                */</span>                <span class="token number">5</span><span class="token punctuation">.</span><span class="token function">invokeBeanFactoryPostProcessors</span><span class="token punctuation">(</span>beanFactory<span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token comment" spellcheck="true">// Register bean processors that intercept bean creation.</span>                <span class="token comment" spellcheck="true">/*注册BeanPostProcessor Bean的后置处理器[把BeanPostProcessor添加到beanFactory],拦截Bean的注册过程                在Bean的创建不同时期执行不同的BeanPostProcessor 及子类                */</span>                <span class="token number">6</span><span class="token punctuation">.</span><span class="token function">registerBeanPostProcessors</span><span class="token punctuation">(</span>beanFactory<span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token comment" spellcheck="true">// Initialize message source for this context.</span>                <span class="token comment" spellcheck="true">//初始化MessageSource 国际化功能:消息绑定 消息解析</span>                <span class="token comment" spellcheck="true">//MessageSource 取出国际化配置文件中的某key:按照区域信息获取</span>                <span class="token number">7</span><span class="token punctuation">.</span><span class="token function">initMessageSource</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token comment" spellcheck="true">// Initialize event multicaster for this context.</span>                <span class="token comment" spellcheck="true">/*                初始化事件派发器                */</span>                <span class="token number">8</span><span class="token punctuation">.</span><span class="token function">initApplicationEventMulticaster</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token comment" spellcheck="true">// Initialize other special beans in specific context subclasses.</span>                <span class="token comment" spellcheck="true">// 留给子容器 子类重写该方法 在容器刷新的时候可以自定义逻辑</span>                <span class="token number">9</span><span class="token punctuation">.</span><span class="token function">onRefresh</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token comment" spellcheck="true">// Check for listener beans and register them.</span>                <span class="token comment" spellcheck="true">// 给容器中将所有ApplicationListener添加到ApplicationEventMulticaster中,注册进来</span>                <span class="token number">10</span><span class="token punctuation">.</span><span class="token function">registerListeners</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token comment" spellcheck="true">// Instantiate all remaining (non-lazy-init) singletons.</span>                <span class="token comment" spellcheck="true">/*                preInstantiateSingletons()初始化所有生下来的单实例Bean                1)获取容器中所有的Bean 依次进行初始化和创建对象                2)获取Bean的定义信息:RootBeanDefinition                3)判断Bean不是抽象的 是单实例的 不是懒加载的                   1),判断是否是FactoryBean 是不是实现了FactoryBean接口的Bean                   2),不是工厂Bean 就用getBean(beanname)创建对象                        // Give BeanPostProcessors a chance to return a proxy instead of the target bean instance.                         resolveBeforeInstantiation(beanName, mbdToUse);                         postProcessBeforeInstantiation                */</span>                <span class="token number">11</span><span class="token punctuation">.</span><span class="token function">finishBeanFactoryInitialization</span><span class="token punctuation">(</span>beanFactory<span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token comment" spellcheck="true">// Last step: publish corresponding event.</span>                <span class="token comment" spellcheck="true">//完成BeanFactory的初始化创建工作 也就是IOC容器创建成功</span>                <span class="token function">finishRefresh</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>            <span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">BeansException</span> ex<span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">if</span> <span class="token punctuation">(</span>logger<span class="token punctuation">.</span><span class="token function">isWarnEnabled</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                    logger<span class="token punctuation">.</span><span class="token function">warn</span><span class="token punctuation">(</span><span class="token string">"Exception encountered during context initialization - "</span> <span class="token operator">+</span>                            <span class="token string">"cancelling refresh attempt: "</span> <span class="token operator">+</span> ex<span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span>                <span class="token comment" spellcheck="true">// Destroy already created singletons to avoid dangling resources.</span>                <span class="token function">destroyBeans</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token comment" spellcheck="true">// Reset 'active' flag.</span>                <span class="token function">cancelRefresh</span><span class="token punctuation">(</span>ex<span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token comment" spellcheck="true">// Propagate exception to caller.</span>                <span class="token keyword">throw</span> ex<span class="token punctuation">;</span>            <span class="token punctuation">}</span>            <span class="token keyword">finally</span> <span class="token punctuation">{</span>                <span class="token comment" spellcheck="true">// Reset common introspection caches in Spring's core, since we</span>                <span class="token comment" spellcheck="true">// might not ever need metadata for singleton beans anymore...</span>                <span class="token function">resetCommonCaches</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> spring </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spring容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx.conf</title>
      <link href="/2020/01/04/nginx-conf/"/>
      <url>/2020/01/04/nginx-conf/</url>
      
        <content type="html"><![CDATA[<pre><code>user  root;              #nginx配置用户或者组 ,启动nginx的权限.默认nobodyworker_processes  8;     #并发服务,允许生成的进程数 默认为 1 [number/auto]error_log  logs/error.log;  #制定日志路径，级别。这个设置可以放入全局块，http块，server块，级别以此为：debug|info|notice|warn|error|crit|alert|emerg    #error_log  logs/error.log  notice;#error_log  logs/error.log  info;#pid        logs/nginx.pid;   #指定nginx进程运行文件存放地址events { #配置虚拟主机的相关参数，一个http中可以有多个server。     accept_mutex on;            #设置网路连接序列化，防止惊群现象发生，默认为on    multi_accept on;              #设置一个进程是否同时接受多个网络连接，默认为off    #use epoll;                    #事件驱动模型，select|poll|kqueue|epoll|resig|/dev/poll|eventport    worker_connections  1024;   #最大连接数，默认为512}http {#可以嵌套多个server，配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置    include       mime.types;   #网络资源的请求类型[文件扩展名与文件类型映射表]    default_type  application/octet-stream; #默认文件类型，默认为text/plain    log_format  myFormat   &#39;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#39;                           &#39;$status $body_bytes_sent &quot;$http_referer&quot; &#39;                           &#39;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#39;; #自定义格式日志格式    access_log log/access.log myFormat;  #combined为日志格式的默认值    #access_log  off; #取消服务日志    sendfile        on;   #数据传输配置:允许sendfile方式传输文件，默认为off，可以在http块，server块，location块。    sendfile_max_chunk 100k;  #每个进程每次调用传输数量不能大于设定的值，默认为0，即不设上限。    #tcp_nopush     on;      # keepalive_timeout  0;   #限制某连接单个请求的上线    keepalive_timeout  65;   #连接超时时间，默认为75s，可以在http，server，location块。    #gzip  on; #gzip请求配置      include config/customer-portal.conf; #include配置文件的引入    error_page 404 https://www.baidu.com; #错误页     #负载均衡 upstream onelife{              server   10.28.85.132:9086 weight=1 max_fails=2 fail_timeout=900s;              server   192.168.10.121:3333 backup;  #热备    }     upstream imodule-sso-server{              server   10.28.85.134:9080 weight=1 max_fails=2 fail_timeout=900s;    } #    client_header_buffer_size 10240k;#    large_client_header_buffers 4 10240k;# 在server 指令中,我们同样可以配置多个 location 指令location 指令能将我们的字符串请求解析到对应的IP和端口,从而去获取正确的资源,location 也可以进行特殊配置,定制 网站的 404 ,500 等页面.    server {        keepalive_requests 120; #单连接请求上限次数。            listen 80;   #监听端口        server_name 10.28.85.130;  #监听地址        charset utf-8;#编码方式        location / {           root   html;           index  index.html index.htm;        }        error_page   500 502 503 504  /50x.html;        location = /50x.html {             root   html; #根目录            #index vv.txt;  #设置默认页        }        location /onelife { #~*^.+$#     #请求的url过滤，正则匹配，~为区分大小写，~*为不区分大小写。            proxy_pass   http://onelife;    #请求转向onelife定义的服务器列表            proxy_set_header  Host       $http_host;            proxy_set_header  Cookie     $http_cookie;            proxy_set_header  X-Real-IP  $remote_addr;            proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;            #client_max_body_size  300m;            expires -1;            #proxy_read_timeout 150;            client_max_body_size 300m;            client_body_buffer_size 256k;            proxy_connect_timeout 600;            proxy_read_timeout 600;            proxy_send_timeout 600;            proxy_buffer_size 128k;            proxy_buffers   4 64k;            proxy_busy_buffers_size 128k;            proxy_temp_file_write_size 128k;            break;        }        location /imodule-sso-server {            proxy_pass   http://imodule-sso-server;   #请求转向onelife定义的服务器列表            deny  127.0.0.1;  #拒绝的ip            allow 172.18.5.54; #允许的ip            proxy_set_header  Host       $http_host;            proxy_set_header  Cookie     $http_cookie;            proxy_set_header  X-Real-IP  $remote_addr;            proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;            #client_max_body_size  300m;            expires -1;            #proxy_read_timeout 150;            client_max_body_size 300m;            client_body_buffer_size 256k;            proxy_connect_timeout 600;            proxy_read_timeout 600;            proxy_send_timeout 600;            proxy_buffer_size 128k;            proxy_buffers   4 64k;            proxy_busy_buffers_size 128k;            proxy_temp_file_write_size 128k;            break;        }    }       }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> Nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx配置 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jasper-PDF 更改字体</title>
      <link href="/2019/12/29/jasper-PDF%20%E6%9B%B4%E6%94%B9%E5%AD%97%E4%BD%93/"/>
      <url>/2019/12/29/jasper-PDF%20%E6%9B%B4%E6%94%B9%E5%AD%97%E4%BD%93/</url>
      
        <content type="html"><![CDATA[<h3 id="开发环境-Windows"><a href="#开发环境-Windows" class="headerlink" title="开发环境(Windows)"></a>开发环境(Windows)</h3><pre><code>1. 使用公司规定的字体. 引用要求的字体jar包</code></pre><p><img src="http://47.105.145.116/pictures/jasper%E5%AD%97%E4%BD%93.png" alt=""></p><pre><code>2.japser 只能加载.ttf的字体,本地JasperStudio 制作时可以设置 PDF Font Name =C:\Windows\Fonts\msyh.ttf(微软雅黑)进行本地开发.</code></pre><h3 id="Linux环境"><a href="#Linux环境" class="headerlink" title="Linux环境"></a>Linux环境</h3><p>第一步:在classpath路径下 添加字体.</p><hr><p>第二步:</p><p>PDF Embedded = false</p><p>PDF Encodong = IDentity-H(Unicode with horizontal writing)</p><p>PDF Font Name =classpath:file\msyh.ttf</p><p><img src="http://47.105.145.116/pictures/fontpath.png" alt=""></p><hr><p>第三步:</p><p>在classpath路径下添加 jasperreports.properties 属性文件 不然会报改字体找不到错误</p><p>文件内容为： </p><p>net.sf.jasperreports.awt.ignore.missing.font=true </p>]]></content>
      
      
      <categories>
          
          <category> Jasper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JasperPDF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nginx 访问文件路径</title>
      <link href="/2019/12/26/Nginx-%E8%AE%BF%E9%97%AE%E6%96%87%E4%BB%B6%E8%B7%AF%E5%BE%84/"/>
      <url>/2019/12/26/Nginx-%E8%AE%BF%E9%97%AE%E6%96%87%E4%BB%B6%E8%B7%AF%E5%BE%84/</url>
      
        <content type="html"><![CDATA[<h1 id="root-和-alias的区别"><a href="#root-和-alias的区别" class="headerlink" title="root 和 alias的区别"></a>root 和 alias的区别</h1><p>nginx 目录结构</p><p>/usr/local/nginx 根目录</p><blockquote><p>|—-html</p></blockquote><blockquote><blockquote><p>|—–hexo</p></blockquote></blockquote><blockquote><blockquote><blockquote><p>|—–index.html</p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>|—–iamge</p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><blockquote><p>|—–xo.jpg</p></blockquote></blockquote></blockquote></blockquote><h2 id="root"><a href="#root" class="headerlink" title="root"></a>root</h2><pre><code>location / {    root html/hexo    index index.html}</code></pre><p>浏览器地址栏访问<br><a href="http://47.105.145:80/" target="_blank" rel="noopener">http://47.105.145:80/</a></p><p>实际路径:/usr/local/nginx/hexo/indexhtml</p><pre><code>location /hexo {    root  html    index index.html}</code></pre><p>浏览器地址栏访问<br><a href="http://47.105.145:80/hexo" target="_blank" rel="noopener">http://47.105.145:80/hexo</a></p><p>实际路径:/usr/local/nginx/hexo/indexhtml</p><pre><code>location /路径名 必须是root 路径下的子路径root: 配置的路径html+访问的/hexo 构成访问的静态资源路径</code></pre><hr><h2 id="alias"><a href="#alias" class="headerlink" title="alias"></a>alias</h2><pre><code>location /imag {    alias  html/hexo/imag    index  xo.jpg}</code></pre><p>浏览器地址栏访问:<a href="http://47.105.145:80/imag" target="_blank" rel="noopener">http://47.105.145:80/imag</a></p><p>实际路径:/usr/local/nginx/hexo/imag/xo.jpg</p><pre><code>location /love {    alias  html/hexo/imag    index  xo.jpg}</code></pre><p>浏览器地址栏访问:<a href="http://47.105.145:80/love" target="_blank" rel="noopener">http://47.105.145:80/love</a></p><p>实际路径:/usr/local/nginx/hexo/imag/xo.jpg</p><pre><code>web服务器将会返回服务器上 /usr/local/nginx/hexo/imag/xo.jpg注意这里是imag，因为alias会把location后面配置的路径丢弃掉，跟love 没有关系,把当前匹配到的目录指向到指定的目录。</code></pre>]]></content>
      
      
      <categories>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ngix静态访问 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo介绍</title>
      <link href="/2019/12/19/hexo%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/12/19/hexo%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo使用介绍 </tag>
            
            <tag> hexo主题 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
